{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of WordNet\n",
        "\n",
        "[WordNet](https://wordnet.princeton.edu/) a project that provides a large lexical database of English nouns, verbs, adjectives and adverbs, which are grouped into synsets, which are sets of \"cognitive synonyms\" [1]. The intersting thing about WordNet is that it also contains information on the hierarchical relationships between these synsets, making it a powerful tool for natural language processing (NLP) applications.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Princeton University \"About WordNet.\" WordNet. Princeton University. 2010."
      ],
      "metadata": {
        "id": "V6t5eIRrCIQN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxrWrCRSCGCW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.book import *\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloads\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyEkuCyecv23",
        "outputId": "8430a213-fa7d-4275-8ed8-821c97ec3b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all synsets of 'painting'\n",
        "wn.synsets('painting', pos=wn.NOUN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KPn2ThZrR3-",
        "outputId": "19c76a27-a86e-4a63-b0ff-d60ec8089292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('painting.n.01'),\n",
              " Synset('painting.n.02'),\n",
              " Synset('painting.n.03'),\n",
              " Synset('painting.n.04')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a definition for the first noun in synset\n",
        "wn.synset('painting.n.01').definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7pHpEfgWxgdE",
        "outputId": "cba9e8ff-ea07-40d0-93fc-6feaf6cc6cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'graphic art consisting of an artistic composition made by applying paints to a surface'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract usage examples\n",
        "wn.synset('painting.n.01').examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYApzIWEyVOc",
        "outputId": "d02670af-e94e-4e42-fbd5-27f39e60fa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a small painting by Picasso',\n",
              " 'he bought the painting as an investment',\n",
              " 'his pictures hang in the Louvre']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract lemmas\n",
        "wn.synset('painting.n.01').lemmas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_SNGvj7ygE0",
        "outputId": "56f75fff-0fba-4448-bc74-c61e6e34150d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('painting.n.01.painting'), Lemma('painting.n.01.picture')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traverse up the WordNet hierarchy \n",
        "hyper = lambda s: s.hypernyms()\n",
        "list(wn.synset('painting.n.01').closure(hyper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Y-cZ34zHdT",
        "outputId": "d8445145-38f0-4ba7-fbd3-6689d0e7ab05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('graphic_art.n.01'),\n",
              " Synset('art.n.01'),\n",
              " Synset('creation.n.02'),\n",
              " Synset('artifact.n.01'),\n",
              " Synset('whole.n.02'),\n",
              " Synset('object.n.01'),\n",
              " Synset('physical_entity.n.01'),\n",
              " Synset('entity.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis on how WordNet organizes nouns\n",
        "Nouns are organized in a hierarchical structure with the synset 'entity.n.01' at the top, analogous to how many object-oriented programming languages employ a super-type 'Object' to serve as the parent of all other types. \n",
        "\n",
        "Nouns are grouped into synsets, each synset representing a group of words that have the same meaning and POS. The same spelling for a word may have different synsets for each meaning that the word has."
      ],
      "metadata": {
        "id": "YbdU5t6b4j9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output hypernyms (higher)\n",
        "hyper = [h.name() for h in wn.synset('painting.n.01').hypernyms()]\n",
        "print(\"Hypernyms:\", hyper)\n",
        "\n",
        "# output hyponyms (lower)\n",
        "hypo = [h.name() for h in wn.synset('painting.n.01').hyponyms()]\n",
        "print(\"Hyponyms:\", hypo)\n",
        "\n",
        "# output meronyms (part of)\n",
        "mero = [h.name() for h in wn.synset('painting.n.01').part_meronyms()]\n",
        "print(\"Meronyms:\", mero)\n",
        "\n",
        "# output holonyms (whole)\n",
        "holo = [h.name() for h in wn.synset('painting.n.01').part_holonyms()]\n",
        "print(\"Holonyms:\", holo)\n",
        "\n",
        "# output antonyms\n",
        "ant = []\n",
        "for lemma in wn.synset('painting.n.01').lemmas():\n",
        "  for antonym in lemma.antonyms():\n",
        "    ant.append(antonym.name())\n",
        "    \n",
        "print(\"Antonyms:\", ant)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEyDTVMt67if",
        "outputId": "fc12d0dd-782b-4b70-db3a-fe64c727910a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms: ['graphic_art.n.01']\n",
            "Hyponyms: ['abstraction.n.04', 'cityscape.n.02', 'daub.n.03', 'distemper.n.04', 'finger-painting.n.01', 'icon.n.03', 'landscape.n.02', 'miniature.n.01', 'monochrome.n.01', 'mural.n.01', 'nude.n.01', 'oil_painting.n.01', 'pentimento.n.01', 'sand_painting.n.01', 'seascape.n.02', 'semi-abstraction.n.01', 'still_life.n.01', 'tanka.n.02', \"trompe_l'oeil.n.01\", 'watercolor.n.01']\n",
            "Meronyms: []\n",
            "Holonyms: []\n",
            "Antonyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all synsets of 'paint' verb\n",
        "wn.synsets('painting', pos=wn.VERB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v89ThVXNARoq",
        "outputId": "60f8e2b9-cc14-444f-a9bf-770c09378912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('paint.v.01'),\n",
              " Synset('paint.v.02'),\n",
              " Synset('paint.v.03'),\n",
              " Synset('paint.v.04')]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.synsets('painting', pos=wn.VERB)[0])\n",
        "\n",
        "# extract definition\n",
        "definition = wn.synset('paint.v.01').definition()\n",
        "print(\"Definition:\", definition)\n",
        "\n",
        "# extract usage examples\n",
        "usage_examples = wn.synset('paint.v.01').examples()\n",
        "print(\"Usage examples:\", usage_examples)\n",
        "\n",
        "# extract lemmas\n",
        "lemmas = wn.synset('paint.v.01').lemmas()\n",
        "print(\"Lemmas:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIi8Wd53BCrB",
        "outputId": "31e87905-7ea1-450b-bfe1-dd5beb630e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('paint.v.01')\n",
            "Definition: make a painting\n",
            "Usage examples: ['he painted all day in the garden', 'He painted a painting of the garden']\n",
            "Lemmas: [Lemma('paint.v.01.paint')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# traverse up hierarchy\n",
        "hyp = lambda s: s.hypernyms()\n",
        "hierarchy = list(wn.synset('paint.v.01').closure(hyp))\n",
        "print(\"Hierarchy of 'paint.v.01':\", hierarchy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJyDa8e2CNId",
        "outputId": "3a3598c8-a81c-4db5-eb97-7b3c252a3b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hierarchy of 'paint.v.01': [Synset('create.v.03'), Synset('act.v.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis on how WordNet is organized for verbs\n",
        "As opposed to the hierarchy of nouns in WordNet, verbs do not have a top level synset. \n",
        "\n",
        "The synset for verbs also seem to be defined using the base form of the verb, also known as the present infinitive tense. For example, `wn.synsets('painting', pos=wn.VERB)` returned synsets in the form 'paint.v.XX'"
      ],
      "metadata": {
        "id": "DchjrVUyC3_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "word = \"remembering\"\n",
        "synsets = wn.synsets(word)\n",
        "\n",
        "s = set()\n",
        "for synset in synsets:\n",
        "  lemmas = synset.lemmas()\n",
        "  for lemma in lemmas:\n",
        "    forms = lemma.derivationally_related_forms()\n",
        "    for form in forms:\n",
        "      name = form.name()\n",
        "      if stemmer.stem(name) == stemmer.stem(word):\n",
        "        s.add(name)\n",
        "\n",
        "print(s)"
      ],
      "metadata": {
        "id": "P6tPQhZRD0Zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b63ef89-1f82-41bb-fb01-ce6d032acfc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'remembering', 'remember'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select two words that you think might be similar. Find the specific synsets you are interested in.\n",
        "word1 = wn.synsets(\"goal\")[0]\n",
        "word2 = wn.synsets(\"aim\")[1]\n",
        "\n",
        "print(word1, \":\", word1.definition())\n",
        "print(word2, \":\", word2.definition())\n",
        "\n",
        "# Wu-Palmer similarity Metric\n",
        "print(\"Wu-Palmer Similarity:\", wn.wup_similarity(word1, word2))\n",
        "\n",
        "# Run the Lesk Algorithm,\n",
        "print(\"\\nRunning the Lesk algorith\")\n",
        "sent1 = [\"The\", \"goal\", \"is\", \"to\", \"pass\", \"the\", \"class\"] \n",
        "sent2 = [\"The\", \"aim\", \"is\", \"to\", \"pass\", \"the\", \"class\"]\n",
        "sent3 = [\"He\", \"took\", \"aim\", \"and\", \"fired\"]\n",
        "\n",
        "print(lesk(sent1, \"goal\", \"n\"))\n",
        "print(lesk(sent2, \"aim\", \"n\"))\n",
        "print(lesk(sent3, \"aim\", \"n\"))\n",
        "print()\n",
        "\n",
        "print(\"synset('purpose.n.01'):\", wn.synset('purpose.n.01').definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KodWLgfjPhv",
        "outputId": "f2443f6f-492f-4d3b-f80a-448c1d62563d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('goal.n.01') : the state of affairs that a plan is intended to achieve and that (when achieved) terminates behavior intended to achieve it\n",
            "Synset('aim.n.02') : the goal intended to be attained (and which is believed to be attainable)\n",
            "Wu-Palmer Similarity: 0.9230769230769231\n",
            "\n",
            "Running the Lesk algorith\n",
            "Synset('goal.n.01')\n",
            "Synset('aim.n.02')\n",
            "Synset('purpose.n.01')\n",
            "\n",
            "synset('purpose.n.01'): an anticipated outcome that is intended or that guides your planned actions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Observations on the Wu-Palmer metric and the Lesk Algorithm\n",
        "I used the words 'goal' and 'aim' to see if the multiple meanings of the word 'aim' would confuse either of these algorithms. \n",
        "\n",
        "Specifically, I used the following synsets:\n",
        "- Synset('goal.n.01') : the state of affairs that a plan is intended to achieve and that (when achieved) terminates behavior intended to achieve it\n",
        "- Synset('aim.n.02') : the goal intended to be attained (and which is believed to be attainable)\n",
        "\n",
        "\n",
        "The Wu-Palmer similarity metric scored a `0.9230769230769231`, indicating that it correctly identified the similarity between the two synsets. \n",
        "\n",
        "The Lesk algorithm, however, does not take in a synset, but rather just a word. I tried to confuse it by using the word 'aim', which can have an ambiguous meaning depending on the context. In the sentnces \"The goal/aim is to pass the class\", the Lesk algorithm correctly identified the synsets I intended it to. In the sentence \"he took aim and fired\", however, the Lesk algorithm returned `synset('purpose.n.01'): an anticipated outcome that is intended or that guides your planned actions`, which was not the intended meaning. "
      ],
      "metadata": {
        "id": "U4kSXnZvfcXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # My analysis of SentiWordNet\n",
        " Similar to WordNet's `synset`, SentiWordNet works with a `senti_synset` for a word. Each synset has a `positive`, `negative`, and `objectivity` score. It is important to note that these scores are assigned to a `senti_synset`, and thus SentiWordNet does not take into account a words context in the sentence or corpus as a whole, making it less accurate than tools such as [Vader](https://github.com/cjhutto/vaderSentiment).\n",
        "\n",
        " Still, I believe that SentiWordNet has many possible use cases, such as the following: \n",
        " - Automatically assigning ratings to movies based on mentions in social media applications. For example one could use Twitter's API to get all tweets that mention the movie, and then aggregate the SentiWordNet scores of those tweets to gage whether the reviews are mostly positive or mostly negative.\n",
        " - Help a company gage the public opinion of their brand. One could make a Web Crawler that searches for mentions of the company on the Web, and then aggregates positivity and negativity scores to gage the public opinion on the company. This product could even single out some of the most negative mentions and report those to the company, allowing their PR team to focus on the main areas of concern\n",
        "\n"
      ],
      "metadata": {
        "id": "EyQ1td1UiWGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "syn_list = list(swn.senti_synsets(\"hunger\"))\n",
        "for syn in syn_list:\n",
        "  print(syn)\n",
        "\n",
        "print()\n",
        "\n",
        "sent = \"we have to solve world hunger it is unnaceptable that people are still dying of starvation\"\n",
        "tokens = sent.split()\n",
        "for token in tokens:\n",
        "  syn_list = list(swn.senti_synsets(token))\n",
        "  if syn_list:\n",
        "    print(token, \": \", syn_list[0])\n",
        "\n",
        "print()\n",
        "\n",
        "# using Lesk and POS tagging\n",
        "# sent = \"we have to solve world hunger it is unnaceptable that people are still dying of starvation\"\n",
        "# tokens = nltk.word_tokenize(sent)\n",
        "# tags = nltk.pos_tag(tokens)\n",
        "# for tag in tags:\n",
        "#   print(tag)\n",
        "#   synset = lesk(sent, tag[0], tag[1]) \n",
        "#   print(synset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXd8Yz0dmAod",
        "outputId": "ad0fead1-9168-4112-db52-ee72f4c9b8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<hunger.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<hunger.n.02: PosScore=0.25 NegScore=0.375>\n",
            "<hunger.v.01: PosScore=0.0 NegScore=0.125>\n",
            "<crave.v.01: PosScore=0.5 NegScore=0.0>\n",
            "<starve.v.01: PosScore=0.0 NegScore=0.25>\n",
            "\n",
            "have :  <rich_person.n.01: PosScore=0.0 NegScore=0.0>\n",
            "solve :  <solve.v.01: PosScore=0.0 NegScore=0.0>\n",
            "world :  <universe.n.01: PosScore=0.0 NegScore=0.0>\n",
            "hunger :  <hunger.n.01: PosScore=0.0 NegScore=0.0>\n",
            "it :  <information_technology.n.01: PosScore=0.0 NegScore=0.0>\n",
            "is :  <be.v.01: PosScore=0.25 NegScore=0.125>\n",
            "people :  <people.n.01: PosScore=0.0 NegScore=0.0>\n",
            "are :  <are.n.01: PosScore=0.0 NegScore=0.0>\n",
            "still :  <still.n.01: PosScore=0.0 NegScore=0.0>\n",
            "dying :  <death.n.04: PosScore=0.0 NegScore=0.625>\n",
            "starvation :  <starvation.n.01: PosScore=0.125 NegScore=0.0>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Observation on SentiWordNet Scores and their Utility in NLP Applications\n",
        "I found it interesting that some of the synsets of \"hunger\" have a 0 `pos` and `neg` score, while others do not. This highlights the fact that, in order for SentiWordNet to be useful, you need to first identify the correct synset for every word in a sentence. \n",
        "\n",
        "I believe that SentiWordNet cannot be used to its full capabilty without combining it with something like the Lesk algorithm and POS tagging in order to be able to identity the most probable `senti_synset`. That being said, if integrated correctly with these other tools, I believe that SentiWordNet can be very powerful in detecting the positivity or negativity of a piece of text. Knowing this positivity/negativity information has some intrinsic value and can be directly applied, but it can also be used in conjunction with other NLP tools to get more meaningful insights.\n",
        "\n",
        "Another limitation of SentiWordNet is that it is not very nuanced. It is not able detect the difference between specific emotions like sadness and anger, relief, and excitement, but rather lumps things into either being positive or negative."
      ],
      "metadata": {
        "id": "FTB3ouasofEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Are Collocations\n",
        "Collocations are word that combine to have a meaning that is greater than the sum of its parts. They are often well-known phrases in a language, and can be hard to undertand by someone that is not from that culture or by NLP applications."
      ],
      "metadata": {
        "id": "f7UjDjuVwYCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output collocations of text 4\n",
        "print(text4.collocations())\n",
        "\n",
        "# calculate the mutual information for 'Old World'\n",
        "text = ' '.join(text4.tokens)\n",
        "vocab_len = len(set(text))\n",
        "\n",
        "p_ol = text.count('Old World') / vocab_len\n",
        "print(\"p(Old World) =\", p_ol)\n",
        "\n",
        "p_o = text.count('Old') / vocab_len\n",
        "print(\"p(Old) =\", p_o)\n",
        "\n",
        "p_w = text.count('World') / vocab_len\n",
        "print(\"p(World) = \", p_w)\n",
        "\n",
        "pmi = math.log2(p_ol / (p_o * p_w))\n",
        "print(\"pmi =\", pmi)\n",
        "\n",
        "print()\n",
        "\n",
        "# calculate the mutual information for 'the citizens'\n",
        "text = ' '.join(text4.tokens)\n",
        "vocab_len = len(set(text))\n",
        "\n",
        "p_ol = text.count('the citizens') / vocab_len\n",
        "print(\"p(the citizens) =\", p_ol)\n",
        "\n",
        "p_o = text.count('the') / vocab_len\n",
        "print(\"p(the) =\", p_o)\n",
        "\n",
        "p_w = text.count('citizens') / vocab_len\n",
        "print(\"p(Worcitizens) = \", p_w)\n",
        "\n",
        "pmi = math.log2(p_ol / (p_o * p_w))\n",
        "print(\"pmi =\", pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xle4V3fvumz7",
        "outputId": "328a8144-ccf3-47de-ec12-e3e7f2ad3182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "None\n",
            "p(Old World) = 0.11904761904761904\n",
            "p(Old) = 0.13095238095238096\n",
            "p(World) =  0.21428571428571427\n",
            "pmi = 2.084888897586513\n",
            "\n",
            "p(the citizens) = 0.13095238095238096\n",
            "p(the) = 149.21428571428572\n",
            "p(Worcitizens) =  3.2142857142857144\n",
            "pmi = -11.838625833423038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Commentary on results of the Mutual Information Formula and my Interpretation\n",
        "We can see that 'Old World' has a higher mutual information than 'the citizens', indicating that 'Old World' is more likely to be a collocation. 'the citizens' is simply a bigram that happens to be common, in fact any bigram in the form 'the NOUN' is really common in English. 'Old World', however, carries with it a specific meaning, which is referencing the Americas, a term that goes back to the age of exploration.  \n",
        "\n",
        "PMI formula: `log_2( P(x, y) / ( P(x) * P(y) ) )`\n",
        "\n",
        "The PMI formula helps distinguish between common bigrams and collocations by taking into account the frequency of individual words and the probability of them occurring together by chance. In the formula, we take the probability of the bigram occuring, and divide by the product of their individual probablities. Essentially, we taking the likelihood of the bigram occuring, and dividing by the likelihood of this happening by chance, thus a higher score would mean that the words occur together more frequently than would be expected. In the simple bigram 'the citizens', the word 'the' is really common, thus making the denominator of the PMI formula larger, and giving us a lower score. We take the log of these probabilities in order to make the scores easier to interpret, scaling down large probabilities and making very small probabliites negative. \n",
        "\n"
      ],
      "metadata": {
        "id": "NUC2W_QN1C1R"
      }
    }
  ]
}